{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a548ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "all_accuracies = []\n",
    "all_f1_scores = []\n",
    "all_test_results = []\n",
    "all_results = []\n",
    "user_folders = [d for d in os.listdir(data_root) \n",
    "                if os.path.isdir(os.path.join(data_root, d)) and \n",
    "                d.endswith('_results_final')]\n",
    "\n",
    "user_initials = [d.split('_results_final')[0] for d in user_folders]\n",
    "\n",
    "validation_dir='DTT_results_final/DTT_inverted'\n",
    "\n",
    "print(\"User folders:\", user_folders)\n",
    "print(\"User initials:\", user_initials)\n",
    "\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "for test_user in user_initials:\n",
    "    if test_user == \"DTT\":\n",
    "        print(\"SKIPPING USER DTT because he is part of validation set!\")\n",
    "        continue\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"LEAVE-ONE-USER-OUT: Testing on user {test_user}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    train_dir, test_dir = get_user_split_paths(test_user)\n",
    "    \n",
    "    if not os.path.exists(train_dir) or not os.path.exists(test_dir):\n",
    "        print(f\"Split directories for {test_user} not found!!!\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Train directory: {train_dir}\")\n",
    "    print(f\"Test directory: {test_dir}\")\n",
    "    \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    test_generator = datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {train_generator.samples} TRAINING images belonging to {train_generator.num_classes} classes.\")\n",
    "    print(f\"Found {test_generator.samples} TEST images belonging to {test_generator.num_classes} classes.\")\n",
    "    print(f\"Using {validation_generator.samples} VALIDATION images belonging to {validation_generator.num_classes} classes.\")\n",
    "    \n",
    "    print(f\"Training with {train_generator.samples} images, testing on {test_generator.samples} images.\")\n",
    "    \n",
    "    model = create_model()\n",
    "    model.load_weights('proper_split_server.weights.h5')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    f1_callback = F1ScoreCallback(validation_generator)\n",
    "    \n",
    "    print(f\"\\nTraining model for test user: {test_user}\")\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[f1_callback, early_stopping]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nEvaluating model for test user: {test_user}\")\n",
    "    results = model.evaluate(test_generator, verbose=0)\n",
    "    test_accuracy = results[1]\n",
    "    test_loss = results[0]\n",
    "    final_f1 = f1_callback.f1_scores[-1] if f1_callback.f1_scores else 0.0\n",
    "    \n",
    "    print(f\"Test User {test_user} - Final Accuracy: {test_accuracy:.4f}, Final F1 od callbackot: {final_f1:.4f}\")\n",
    "\n",
    "    print(f\"\\nEvaluating model on test data for user: {test_user}\")\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in range(len(test_generator)):\n",
    "        x_batch, y_batch = test_generator[i]\n",
    "        preds = model.predict(x_batch, verbose=0)\n",
    "        y_true.extend(np.argmax(y_batch, axis=1))\n",
    "        y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "    test_accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    test_f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    test_f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"Test User {test_user} -> Accuracy: {test_accuracy:.4f}, Macro F1: {test_f1_macro:.4f}, Weighted F1: {test_f1_weighted:.4f}\")\n",
    "    \n",
    "    all_test_results.append({\n",
    "        'user': test_user,\n",
    "        'accuracy': float(test_accuracy),\n",
    "        'f1_macro': float(test_f1_macro),\n",
    "        'f1_weighted': float(test_f1_weighted),\n",
    "        'epochs_trained': len(history.history['loss'])\n",
    "    })\n",
    "    \n",
    "    all_accuracies.append(test_accuracy)\n",
    "    all_f1_scores.append(final_f1)\n",
    "    all_results.append({\n",
    "        'user': test_user,\n",
    "        'accuracy': float(test_accuracy),  \n",
    "        'F1 score': float(test_f1_macro),\n",
    "        'f1_weighted': float(test_f1_weighted),\n",
    "        'callback f1_score': float(final_f1),\n",
    "        'loss': float(test_loss),\n",
    "        'training_samples': int(train_generator.samples),\n",
    "        'test_samples': int(test_generator.samples),\n",
    "        'training_history': {\n",
    "            'epochs': len(history.history['loss']),\n",
    "            'final_train_loss': float(history.history['loss'][-1]),\n",
    "            'final_val_loss': float(history.history['val_loss'][-1]) if 'val_loss' in history.history else None,\n",
    "            'final_train_accuracy': float(history.history['accuracy'][-1]),\n",
    "            'final_val_accuracy': float(history.history['val_accuracy'][-1]) if 'val_accuracy' in history.history else None\n",
    "        }\n",
    "    })\n",
    "\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"user: {test_user}\")\n",
    "    print(f\"accuracy: {float(test_accuracy):.4f}\")\n",
    "    print(f\"f1_score: {float(final_f1):.4f}\")\n",
    "    print(f\"loss: {float(test_loss):.4f}\")\n",
    "    print(f\"training_samples: {int(train_generator.samples)}\")\n",
    "    print(f\"test_samples: {int(test_generator.samples)}\")\n",
    "    print(f\"epochs: {len(history.history['loss'])}\")\n",
    "    print(f\"final_train_loss: {float(history.history['loss'][-1]):.4f}\")\n",
    "    print(f\"final_val_loss: {float(history.history['val_loss'][-1]) if 'val_loss' in history.history else 'None'}\")\n",
    "    print(f\"final_train_accuracy: {float(history.history['accuracy'][-1]):.4f}\")\n",
    "    print(f\"final_val_accuracy: {float(history.history['val_accuracy'][-1]) if 'val_accuracy' in history.history else 'None'}\")\n",
    "    \n",
    "    model.save_weights(f\"out_user_{test_user}_result.weights.h5\")\n",
    "    print(f\"Saved weights: out_user_{test_user}_result.weights.h5\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    class_indices = test_generator.class_indices\n",
    "    with open(f\"out_user_{test_user}_class_indices.json\", 'w') as f:\n",
    "        json.dump(class_indices, f)\n",
    "    \n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "all_accuracies = np.array(all_accuracies)\n",
    "all_f1_scores = np.array(all_f1_scores)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CALCULATING CROSS-VALIDATION STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "mean_accuracy = np.mean(all_accuracies)\n",
    "std_accuracy = np.std(all_accuracies)\n",
    "mean_f1 = np.mean(all_f1_scores)\n",
    "std_f1 = np.std(all_f1_scores)\n",
    "min_accuracy = np.min(all_accuracies)\n",
    "max_accuracy = np.max(all_accuracies)\n",
    "min_f1 = np.min(all_f1_scores)\n",
    "max_f1 = np.max(all_f1_scores)\n",
    "\n",
    "print(\"\\nPer-user results:\")\n",
    "for result in all_results:\n",
    "    print(f\"User {result['user']}: Accuracy = {result['accuracy']:.4f}, F1 = {result['f1_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nOverall Statistics:\")\n",
    "print(f\"Mean Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "print(f\"Mean F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Min Accuracy: {min_accuracy:.4f}\")\n",
    "print(f\"Max Accuracy: {max_accuracy:.4f}\")\n",
    "print(f\"Min F1 Score: {min_f1:.4f}\")\n",
    "print(f\"Max F1 Score: {max_f1:.4f}\")\n",
    "print(f\"Accuracy Range: {max_accuracy - min_accuracy:.4f}\")\n",
    "print(f\"F1 Score Range: {max_f1 - min_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESULTS SAVED TO JSON\")\n",
    "\n",
    "print(f\"\\ Summary:\")\n",
    "print(f\"Leave-One-User-Out Cross-Validation Results:\")\n",
    "print(f\"Number of users: {len(all_accuracies)}\")\n",
    "print(f\"Accuracy: {mean_accuracy:.4f} ± {std_accuracy:.4f} (mean ± std)\")\n",
    "print(f\"F1-Score: {mean_f1:.4f} ± {std_f1:.4f} (mean ± std)\")\n",
    "print(f\"Best performing user: {all_results[np.argmax(all_accuracies)]['user']} ({max_accuracy:.4f})\")\n",
    "print(f\"Worst performing user: {all_results[np.argmin(all_accuracies)]['user']} ({min_accuracy:.4f})\")\n",
    "print(f\"Coefficient of Variation (Accuracy): {(std_accuracy/mean_accuracy*100):.2f}%\")\n",
    "\n",
    "print(\"\\nBETTER F1 SCOREE od final resultot!!!:\")\n",
    "accuracies = [r['accuracy'] for r in all_test_results]\n",
    "f1_macros = [r['f1_macro'] for r in all_test_results]\n",
    "f1_weighteds = [r['f1_weighted'] for r in all_test_results]\n",
    "\n",
    "print(f\"Mean Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "print(f\"Mean Macro F1 Score: {np.mean(f1_macros):.4f} ± {np.std(f1_macros):.4f}\")\n",
    "print(f\"Mean Weighted F1 Score: {np.mean(f1_weighteds):.4f} ± {np.std(f1_weighteds):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diplomski_seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
